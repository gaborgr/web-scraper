## ğŸ“š **Book Scraper con Scrapy & MongoDB**

Este proyecto es un pequeÃ±o **web scraper** construido con ğŸ•·ï¸ **Scrapy** y ğŸ’¾ **MongoDB**, creado con fines **educativos y de prÃ¡ctica**.  
Forma parte de mi proceso de aprendizaje de Scrapy y fue desarrollado siguiendo el excelente tutorial de Real Python ğŸ‘‰  
ğŸ”— [Web Scraping With Scrapy and MongoDB](https://realpython.com/web-scraping-with-scrapy-and-mongodb/#debug-and-test-your-scrapy-web-scraper)

---

### ğŸš€ **DescripciÃ³n**

El objetivo de este proyecto fue **extraer informaciÃ³n de libros** desde el sitio [Books to Scrape](https://books.toscrape.com/), una pÃ¡gina diseÃ±ada especialmente para practicar web scraping.

Cada libro contiene informaciÃ³n como:
- ğŸ·ï¸ TÃ­tulo  
- ğŸ’° Precio  
- ğŸ”— URL del producto  

Los datos extraÃ­dos se almacenan en una base de datos **MongoDB**, aplicando un pequeÃ±o **pipeline** para evitar duplicados y manejar la conexiÃ³n de forma eficiente.

---

### âš™ï¸ **TecnologÃ­as utilizadas**

- ğŸ **Python 3.x**
- ğŸ•·ï¸ **Scrapy**
- ğŸƒ **MongoDB**
- ğŸ§© **PyMongo**
- ğŸ§ª **unittest** (para las pruebas del spider)

---

### ğŸ“¦ **Estructura del proyecto**

```text
books/
â”‚
â”œâ”€â”€ books/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ items.py
â”‚ â”œâ”€â”€ pipelines.py
â”‚ â”œâ”€â”€ settings.py
â”‚ â””â”€â”€ spiders/
â”‚ â””â”€â”€ book_spider.py
â”‚
â””â”€â”€ tests/
â””â”€â”€ test_spider.py
```

---

### ğŸ§  **CÃ³mo funciona**

1. **El spider (`book_spider.py`)** inicia en la URL principal del sitio.  
2. **Extrae** los datos de cada libro (tÃ­tulo, precio, URL).  
3. Si hay una pÃ¡gina siguiente â¡ï¸, Scrapy crea una nueva solicitud (`Request`) y se llama recursivamente al mÃ©todo `parse()`.  
4. **El pipeline (`MongoPipeline`)** procesa cada item:
   - Calcula un ID Ãºnico usando `hashlib` (basado en la URL).  
   - Inserta los datos en MongoDB si aÃºn no existen.  
5. Al finalizar, se cierra la conexiÃ³n con la base de datos.

---

### ğŸ§© **EjecuciÃ³n**

1. AsegÃºrate de tener MongoDB en ejecuciÃ³n:
   ```bash 
   mongod
   ```
2. Ejecuta el spider:
   ```bash 
   scrapy crawl book
   ```
3. Verifica los datos en MongoDB:
   ```bash 
   mongo
   use books_db
   db.books.find().pretty()
   ```

---

### ğŸ§ª **Pruebas**

El proyecto incluye **pruebas unitarias** con un HTML de ejemplo para validar que el mÃ©todo `parse()`:
- Extrae correctamente los libros ğŸ§¾  
- Detecta el enlace a la siguiente pÃ¡gina ğŸ”„  

Ejecuta las pruebas con:
```bash
pytest
```

--

### ğŸ’¡ **ConclusiÃ³n**

Este proyecto fue una excelente oportunidad para entender cÃ³mo funciona **Scrapy** en conjunto con **MongoDB**, aplicando conceptos como:
- Pipelines para procesar y almacenar datos.
- Manejo de errores con `errback`.
- Uso eficiente de `yield` para scraping asÃ­ncrono.
- Testing de spiders con HTML simulado.

ğŸ“š Si estÃ¡s comenzando con Scrapy, te recomiendo seguir el tutorial original de **Real Python**, que explica cada paso con claridad y ejemplos prÃ¡cticos:  
ğŸ‘‰ [Web Scraping with Scrapy and MongoDB - Real Python](https://realpython.com/web-scraping-with-scrapy-and-mongodb/#debug-and-test-your-scrapy-web-scraper)

ğŸš€ PrÃ³ximo paso: crear un **scraper propio**, aplicando todo lo aprendido y explorando nuevas formas de almacenar, validar y analizar los datos obtenidos.
